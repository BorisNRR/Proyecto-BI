{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iSfuJPzKV_Ug",
        "outputId": "672ab18f-09bb-4bf6-ff91-d77b2a57d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline guardado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import nltk\n",
        "\n",
        "# Descargar recursos NLTK necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Cargar el dataset\n",
        "df_unfpa = pd.read_excel('./ODScat_345.xlsx', sheet_name='Datos')\n",
        "\n",
        "# Definir las stop words y el lematizador\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Función para eliminar acentos\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
        "\n",
        "# Función de preprocesamiento de texto\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = remove_accents(text)\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r' ', text)  # Eliminar caracteres no ASCII\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\d', '', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Crear un transformer para usar en el pipeline\n",
        "def preprocess_text_data(text_data):\n",
        "    return [preprocess_text(text) for text in text_data]\n",
        "\n",
        "# Aplicar el preprocesamiento al dataframe\n",
        "df_unfpa['Textos_preprocesados'] = df_unfpa['Textos_espanol'].apply(preprocess_text)\n",
        "\n",
        "# Separar los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_unfpa['Textos_preprocesados'], df_unfpa['sdg'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir el pipeline con preprocesamiento y SVM\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessing', FunctionTransformer(preprocess_text_data, validate=False)),\n",
        "    ('vectorizer', TfidfVectorizer(max_features=5000)),\n",
        "    ('classifier', SVC(probability=True))  # Usar probabilidad para obtener estimaciones de confianza\n",
        "])\n",
        "\n",
        "# Entrenar el pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Exportar el pipeline\n",
        "joblib.dump(pipeline, 'unfpa_text_classification_pipeline.pkl')\n",
        "print(\"Pipeline guardado exitosamente.\")\n"
      ]
    }
  ]
}